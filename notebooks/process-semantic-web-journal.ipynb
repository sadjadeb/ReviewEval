{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "with open('../data/raw/semantic-web-journal.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/processed/semantic-web-journal-analysis.csv\"\n",
    "input_file = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "rows = []\n",
    "\n",
    "for paper in data:\n",
    "    paper_id = paper.get(\"id\", \"\").strip()\n",
    "    if paper_id.upper() == \"UNK\" or not paper_id:\n",
    "        continue\n",
    "\n",
    "    paper_date_str = paper.get(\"date\", \"\")\n",
    "    try:\n",
    "        paper_date = datetime.strptime(paper_date_str, \"%m/%d/%Y\")\n",
    "    except Exception:\n",
    "        paper_date = None\n",
    "\n",
    "    for review in paper.get(\"reviews\", []):\n",
    "        reviewer = review.get(\"reviewer\", \"Anonymous\").strip()\n",
    "        review_date_str = review.get(\"date\", \"\").strip()\n",
    "\n",
    "        # Clean review text\n",
    "        review_text = review.get(\"comment\", \"\")\n",
    "        review_text = review_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "        review_suggestion = review.get(\"suggestion\", \"\")\n",
    "\n",
    "        length_words = len(review_text.split())\n",
    "\n",
    "        try:\n",
    "            review_date = datetime.strptime(review_date_str, \"%d/%b/%Y\")\n",
    "            days_to_submit = (review_date - paper_date).days if paper_date else None\n",
    "        except Exception:\n",
    "            days_to_submit = None\n",
    "\n",
    "        rows.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"reviewer\": reviewer,\n",
    "            \"review_date\": review_date_str,\n",
    "            \"review_suggestion\": review_suggestion,\n",
    "            \"length_words\": length_words,\n",
    "            \"days_to_submit\": days_to_submit,\n",
    "            \"review_text\": review_text\n",
    "        })\n",
    "\n",
    "# Save to CSV with proper quoting\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"‚úÖ Cleaned and saved {len(rows)} reviews with full text to review_analysis.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install taaled pylats spacy\n",
    "# English models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "\n",
    "# Spanish models (used as fallback)\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download es_dep_news_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Ensure 'mattr' column exists\n",
    "    if \"mattr\" not in fieldnames:\n",
    "        fieldnames.append(\"mattr\")\n",
    "    # Drop 'mattr_reason' if it exists\n",
    "    if \"mattr_reason\" in fieldnames:\n",
    "        fieldnames.remove(\"mattr_reason\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing MATTR\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        mattr_value = \"\"\n",
    "\n",
    "        try:\n",
    "            cleaned = lats.Normalize(review_text, lats.ld_params_en)\n",
    "            tokens = cleaned.toks\n",
    "            mattr_value = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "        except Exception as e:\n",
    "            mattr_value = \"\"\n",
    "\n",
    "        row[\"mattr\"] = mattr_value\n",
    "        # Remove 'mattr_reason' if it exists in the row\n",
    "        row.pop(\"mattr_reason\", None)\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated file\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Clean MATTR values saved to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk\n",
    "\n",
    "###########################\n",
    "# Apple silicon support\n",
    "# Uninstall current PyTorch version (if any)\n",
    "# !pip uninstall torch -y\n",
    "\n",
    "# Install PyTorch with MPS (Metal Performance Shaders) support\n",
    "# !pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "###########################\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model.eval()\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Load review rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "    if \"question_count\" not in fieldnames:\n",
    "        fieldnames.append(\"question_count\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Detecting Questions\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        question_count = 0\n",
    "\n",
    "        try:\n",
    "            sentences = sent_tokenize(review_text)\n",
    "            for sent in sentences:\n",
    "                inputs = tokenizer(\n",
    "                    sent,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=64,\n",
    "                    padding=True\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                    # Label 0 = question\n",
    "                    if predicted == 0:\n",
    "                        question_count += 1\n",
    "        except Exception as e:\n",
    "            question_count = \"\"\n",
    "\n",
    "        row[\"question_count\"] = question_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Questions counted and saved in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Citation counting logic ---\n",
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "# --- Load CSV and apply ---\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Update for citation_count\n",
    "    if \"citation_count\" not in fieldnames:\n",
    "        fieldnames.append(\"citation_count\")\n",
    "    if \"has_citation\" in fieldnames:\n",
    "        fieldnames.remove(\"has_citation\")  # Remove old 'has_citation' if needed\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Counting Citations\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        citation_count = count_citations(review_text)\n",
    "        row[\"citation_count\"] = citation_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Citation counts added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(output_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    total = 0\n",
    "    with_citations = 0\n",
    "\n",
    "    for row in reader:\n",
    "        total += 1\n",
    "        if row.get(\"citation_count\") == \"2\":\n",
    "            with_citations += 1\n",
    "\n",
    "print(f\"üìÑ Total reviews: {total}\")\n",
    "print(f\"üîç Reviews with citations: {with_citations}\")\n",
    "print(f\"üìä Percentage: {(with_citations / total * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read and process the file\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Add new column if not already there\n",
    "    if \"sentiment_polarity\" not in fieldnames:\n",
    "        fieldnames.append(\"sentiment_polarity\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Analyzing Sentiment\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        try:\n",
    "            blob = TextBlob(review_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "        except Exception:\n",
    "            sentiment = \"\"\n",
    "\n",
    "        row[\"sentiment_polarity\"] = sentiment\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Sentiment polarity added to review_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install convokit\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, download, TextParser, PolitenessStrategies, Classifier, Utterance, Speaker\n",
    "\n",
    "# Step 1: Load training corpus\n",
    "print(\"üì• Downloading training corpus...\")\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "\n",
    "# Step 2: Load review data and convert to Utterances with dummy speakers\n",
    "review_utterances = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    for idx, row in tqdm(enumerate(reader), desc=\"üîß Preparing Utterances\", total=1805):  # Adjust total if needed\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        if review_text:\n",
    "            dummy_speaker = Speaker(id=f\"reviewer_{idx}\")\n",
    "            review_utterances.append(\n",
    "                Utterance(id=str(idx), text=review_text, speaker=dummy_speaker, meta={\"orig_row\": row})\n",
    "            )\n",
    "\n",
    "# Step 3: Build test corpus\n",
    "print(\"üì¶ Building test corpus...\")\n",
    "test_corpus = Corpus(utterances=review_utterances)\n",
    "\n",
    "# Step 4: Parse\n",
    "print(\"üß† Parsing utterances...\")\n",
    "parser = TextParser()\n",
    "parser.transform(train_corpus)\n",
    "parser.transform(test_corpus)\n",
    "\n",
    "# Step 5: Extract politeness strategies\n",
    "print(\"‚ú® Extracting politeness strategies...\")\n",
    "ps = PolitenessStrategies()\n",
    "ps.transform(train_corpus)\n",
    "ps.transform(test_corpus)\n",
    "\n",
    "# Step 6: Train classifier\n",
    "print(\"üéì Training classifier...\")\n",
    "clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'],\n",
    "                 labeller=lambda utt: utt.meta.get(\"Binary\") == 1)\n",
    "clf.fit(train_corpus)\n",
    "clf.transform(test_corpus)\n",
    "\n",
    "# Step 7: Summarize results\n",
    "print(\"üìà Summarizing scores...\")\n",
    "results = clf.summarize(test_corpus)\n",
    "\n",
    "# Step 8: Merge back to CSV rows\n",
    "print(\"üßæ Merging scores into CSV...\")\n",
    "output_rows = []\n",
    "fieldnames = list(reader[0].keys())\n",
    "if \"politeness_score\" not in fieldnames:\n",
    "    fieldnames.append(\"politeness_score\")\n",
    "\n",
    "for utt in tqdm(test_corpus.iter_utterances(), desc=\"üîó Assigning Scores\"):\n",
    "    row = utt.meta[\"orig_row\"]\n",
    "    try:\n",
    "        score = results.loc[utt.id, \"pred_score\"]\n",
    "        row[\"politeness_score\"] = round(score, 4)\n",
    "    except KeyError:\n",
    "        row[\"politeness_score\"] = \"\"\n",
    "    output_rows.append(row)\n",
    "\n",
    "# Step 9: Save\n",
    "print(\"üíæ Saving to review_analysis.csv...\")\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ All done! Politeness scores are now in your CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Load SPECTER model ---\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    if \"similarity_score\" not in fieldnames:\n",
    "        fieldnames.append(\"similarity_score\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing Relevance Score\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        paper_id = row.get(\"paper_id\", \"\").strip()\n",
    "\n",
    "        try:\n",
    "            # Find matching entry in data\n",
    "            matched_entry = next((entry for entry in data if str(entry.get(\"id\", \"\")).strip() == paper_id), None)\n",
    "\n",
    "            if matched_entry:\n",
    "                title = matched_entry.get(\"title\", \"\")\n",
    "                abstract = matched_entry.get(\"abstract\", \"\")\n",
    "                doc_text = f\"{title} {abstract}\"\n",
    "\n",
    "                # Encode document\n",
    "                doc_inputs = tokenizer(doc_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                doc_inputs = {k: v.to(device) for k, v in doc_inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    doc_emb = model(**doc_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "                # Encode review text\n",
    "                review_inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                review_inputs = {k: v.to(device) for k, v in review_inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    review_emb = model(**review_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "                # Cosine similarity\n",
    "                similarity_score = F.cosine_similarity(doc_emb, review_emb).item()\n",
    "                row[\"similarity_score\"] = similarity_score\n",
    "\n",
    "            else:\n",
    "                row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Relevance scores added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    original_fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Insert title and abstract at positions 5 and 6\n",
    "    new_fieldnames = original_fieldnames[:5] + [\"title\", \"abstract\"] + original_fieldnames[5:]\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Adding Title and Abstract (Escaping Newlines)\"):\n",
    "        paper_id = row.get(\"paper_id\", \"\").strip()\n",
    "\n",
    "        # Find matching entry\n",
    "        matched_entry = next((entry for entry in data if str(entry.get(\"id\", \"\")).strip() == paper_id), None)\n",
    "\n",
    "        if matched_entry:\n",
    "            title = matched_entry.get(\"title\", \"\")\n",
    "            abstract = matched_entry.get(\"abstract\", \"\")\n",
    "        else:\n",
    "            title = \"\"\n",
    "            abstract = \"\"\n",
    "\n",
    "        # Escape real newlines in title and abstract\n",
    "        title = title.replace(\"\\r\\n\", \"\\\\n\").replace(\"\\n\", \"\\\\n\")\n",
    "        abstract = abstract.replace(\"\\r\\n\", \"\\\\n\").replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "        # Build new row\n",
    "        new_row = {}\n",
    "        for idx, field in enumerate(new_fieldnames):\n",
    "            if field == \"title\":\n",
    "                new_row[field] = title\n",
    "            elif field == \"abstract\":\n",
    "                new_row[field] = abstract\n",
    "            else:\n",
    "                # Map original fields\n",
    "                original_field_idx = idx if idx < 5 else idx - 2  # Adjust because we inserted 2 fields\n",
    "                if original_field_idx < len(original_fieldnames):\n",
    "                    original_field = original_fieldnames[original_field_idx]\n",
    "                    new_row[field] = row.get(original_field, \"\")\n",
    "\n",
    "        output_rows.append(new_row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=new_fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Title and Abstract (with clean \\\\n) added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper: parse dates consistently\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%d/%b/%Y\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    rows = list(reader)\n",
    "    fieldnames = list(rows[0].keys())\n",
    "\n",
    "    if \"num_days_before_deadline\" not in fieldnames:\n",
    "        fieldnames.append(\"num_days_before_deadline\")\n",
    "\n",
    "    # First: find latest review_date per paper_id\n",
    "    latest_review_dates = {}\n",
    "\n",
    "    for row in rows:\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        review_date = parse_date(row[\"review_date\"])\n",
    "\n",
    "        if paper_id and review_date:\n",
    "            if paper_id not in latest_review_dates:\n",
    "                latest_review_dates[paper_id] = review_date\n",
    "            else:\n",
    "                if review_date > latest_review_dates[paper_id]:\n",
    "                    latest_review_dates[paper_id] = review_date\n",
    "\n",
    "    # Second: compute days before deadline for each review\n",
    "    for row in tqdm(rows, desc=\"Computing num_days_before_deadline\"):\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        review_date = parse_date(row[\"review_date\"])\n",
    "        deadline_date = latest_review_dates.get(paper_id)\n",
    "\n",
    "        if review_date and deadline_date:\n",
    "            days_before_deadline = (deadline_date - review_date).days\n",
    "            row[\"num_days_before_deadline\"] = days_before_deadline\n",
    "        else:\n",
    "            row[\"num_days_before_deadline\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ num_days_before_deadline added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Scoring Readability\")\n",
    "\n",
    "# Define the readability scoring function\n",
    "def readability_scores(text):\n",
    "    try:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
    "            \"gunning_fog\": textstat.gunning_fog(text),\n",
    "            \"smog_index\": textstat.smog_index(text),\n",
    "            \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": None,\n",
    "            \"flesch_kincaid_grade\": None,\n",
    "            \"gunning_fog\": None,\n",
    "            \"smog_index\": None,\n",
    "            \"automated_readability_index\": None,\n",
    "        }\n",
    "\n",
    "# Apply function with progress bar\n",
    "readability_results = df[\"review_text\"].progress_apply(readability_scores)\n",
    "readability_df = pd.DataFrame(readability_results.tolist())\n",
    "\n",
    "# Merge new columns\n",
    "df = pd.concat([df, readability_df], axis=1)\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved enriched file to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# Define labels used by the HEDGEhog model\n",
    "labels = [\"C\", \"D\", \"E\", \"I\", \"N\"]\n",
    "\n",
    "# Set up model arguments\n",
    "model_args = NERArgs()\n",
    "model_args.labels_list = labels\n",
    "model_args.silent = True\n",
    "model_args.use_multiprocessing = False\n",
    "\n",
    "# Initialize model\n",
    "model = NERModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"jeniakim/hedgehog\",\n",
    "    args=model_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to count each label type\n",
    "def count_hedge_labels(text):\n",
    "    predictions, _ = model.predict([text])\n",
    "    token_labels = [list(token.values())[0] for token in predictions[0]]\n",
    "    counts = Counter(token_labels)\n",
    "    return {label: counts.get(label, 0) for label in labels}\n",
    "\n",
    "# Apply across review_text\n",
    "tqdm.pandas(desc=\"Counting Hedge Labels\")\n",
    "hedge_counts = df[\"review_text\"].progress_apply(count_hedge_labels)\n",
    "\n",
    "# Convert counts into separate columns and join with df\n",
    "hedge_df = pd.DataFrame(hedge_counts.tolist())\n",
    "hedge_df.columns = [f\"hedge_{label}\" for label in hedge_df.columns]\n",
    "\n",
    "df = pd.concat([df.reset_index(drop=True), hedge_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"‚úÖ Hedge label counts saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   0%|          | 0/1805 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 35, attempt 1: No JSON block found\n",
      "‚ùå Error at row 35, attempt 2: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 3: No JSON block found\n",
      "‚ùå Error at row 35, attempt 4: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 5: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 6: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 7: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 8: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 9: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 10: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 11: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 12: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 13: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 35, attempt 14: Invalid \\escape: line 2 column 30 (char 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   2%|‚ñè         | 36/1805 [00:37<30:18,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 35, attempt 15: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 1: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 2: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 3: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 4: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 5: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 6: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 7: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 8: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 9: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 10: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 11: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 12: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 13: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 36, attempt 14: Invalid \\escape: line 2 column 30 (char 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   2%|‚ñè         | 37/1805 [01:20<1:17:38,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 36, attempt 15: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 37, attempt 1: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 37, attempt 2: No JSON block found\n",
      "‚ùå Error at row 37, attempt 3: No JSON block found\n",
      "‚ùå Error at row 37, attempt 4: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 37, attempt 5: No JSON block found\n",
      "‚ùå Error at row 37, attempt 6: No JSON block found\n",
      "‚ùå Error at row 37, attempt 7: No JSON block found\n",
      "‚ùå Error at row 37, attempt 8: No JSON block found\n",
      "‚ùå Error at row 37, attempt 9: No JSON block found\n",
      "‚ùå Error at row 37, attempt 10: No JSON block found\n",
      "‚ùå Error at row 37, attempt 11: No JSON block found\n",
      "‚ùå Error at row 37, attempt 12: No JSON block found\n",
      "‚ùå Error at row 37, attempt 13: No JSON block found\n",
      "‚ùå Error at row 37, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   2%|‚ñè         | 38/1805 [01:37<1:40:21,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 37, attempt 15: No JSON block found\n",
      "‚ùå Error at row 38, attempt 1: No JSON block found\n",
      "‚ùå Error at row 38, attempt 2: No JSON block found\n",
      "‚ùå Error at row 38, attempt 3: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 4: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 5: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 6: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 7: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 8: No JSON block found\n",
      "‚ùå Error at row 38, attempt 9: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 10: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 11: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 12: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 13: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 38, attempt 14: Invalid \\escape: line 2 column 30 (char 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   2%|‚ñè         | 39/1805 [02:00<2:20:10,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 38, attempt 15: Invalid \\escape: line 2 column 30 (char 31)\n",
      "‚ùå Error at row 595, attempt 1: No JSON block found\n",
      "‚ùå Error at row 595, attempt 2: No JSON block found\n",
      "‚ùå Error at row 595, attempt 3: No JSON block found\n",
      "‚ùå Error at row 595, attempt 4: No JSON block found\n",
      "‚ùå Error at row 595, attempt 5: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  33%|‚ñà‚ñà‚ñà‚ñé      | 596/1805 [02:08<01:58, 10.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 678, attempt 1: No JSON block found\n",
      "‚ùå Error at row 678, attempt 2: No JSON block found\n",
      "‚ùå Error at row 678, attempt 3: No JSON block found\n",
      "‚ùå Error at row 678, attempt 4: No JSON block found\n",
      "‚ùå Error at row 678, attempt 5: No JSON block found\n",
      "‚ùå Error at row 678, attempt 6: No JSON block found\n",
      "‚ùå Error at row 678, attempt 7: No JSON block found\n",
      "‚ùå Error at row 678, attempt 8: No JSON block found\n",
      "‚ùå Error at row 678, attempt 9: No JSON block found\n",
      "‚ùå Error at row 678, attempt 10: No JSON block found\n",
      "‚ùå Error at row 678, attempt 11: No JSON block found\n",
      "‚ùå Error at row 678, attempt 12: No JSON block found\n",
      "‚ùå Error at row 678, attempt 13: No JSON block found\n",
      "‚ùå Error at row 678, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  38%|‚ñà‚ñà‚ñà‚ñä      | 679/1805 [02:25<02:11,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 678, attempt 15: No JSON block found\n",
      "‚ùå Error at row 855, attempt 1: No JSON block found\n",
      "‚ùå Error at row 855, attempt 2: No JSON block found\n",
      "‚ùå Error at row 855, attempt 3: No JSON block found\n",
      "‚ùå Error at row 855, attempt 4: No JSON block found\n",
      "‚ùå Error at row 855, attempt 5: No JSON block found\n",
      "‚ùå Error at row 855, attempt 6: No JSON block found\n",
      "‚ùå Error at row 855, attempt 7: No JSON block found\n",
      "‚ùå Error at row 855, attempt 8: No JSON block found\n",
      "‚ùå Error at row 855, attempt 9: No JSON block found\n",
      "‚ùå Error at row 855, attempt 10: No JSON block found\n",
      "‚ùå Error at row 855, attempt 11: No JSON block found\n",
      "‚ùå Error at row 855, attempt 12: No JSON block found\n",
      "‚ùå Error at row 855, attempt 13: No JSON block found\n",
      "‚ùå Error at row 855, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 856/1805 [02:39<01:38,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 855, attempt 15: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 1: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 2: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 3: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 4: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 5: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 6: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 7: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 8: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 9: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 10: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 11: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 12: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 13: No JSON block found\n",
      "‚ùå Error at row 1146, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1147/1805 [02:56<00:55, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 1146, attempt 15: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 1: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 2: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 3: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 4: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 5: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 6: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 7: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 8: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 9: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 10: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 11: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 12: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 13: No JSON block found\n",
      "‚ùå Error at row 1234, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1235/1805 [03:08<00:52, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 1234, attempt 15: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 1: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 2: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 3: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 4: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 5: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 6: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 7: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 8: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 9: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 10: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 11: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 12: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 13: No JSON block found\n",
      "‚ùå Error at row 1300, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1301/1805 [03:25<00:59,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 1300, attempt 15: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 1: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 2: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 3: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 4: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 5: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 6: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 7: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 8: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 9: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 10: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 11: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 12: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 13: No JSON block found\n",
      "‚ùå Error at row 1552, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1553/1805 [03:42<00:23, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 1552, attempt 15: No JSON block found\n",
      "‚ùå Error at row 1626, attempt 1: Expecting value: line 4 column 28 (char 155)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1553/1805 [03:56<00:23, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 1626, attempt 2: Expecting value: line 4 column 28 (char 155)\n",
      "‚ùå Error at row 1626, attempt 3: No JSON block found\n",
      "‚ùå Error at row 1626, attempt 4: No JSON block found\n",
      "‚ùå Error at row 1626, attempt 5: Expecting value: line 4 column 28 (char 155)\n",
      "‚ùå Error at row 1626, attempt 6: No JSON block found\n",
      "‚ùå Error at row 1626, attempt 7: Expecting value: line 4 column 28 (char 155)\n",
      "‚ùå Error at row 1626, attempt 8: Expecting value: line 4 column 28 (char 155)\n",
      "‚ùå Error at row 1626, attempt 9: Expecting value: line 3 column 30 (char 141)\n",
      "‚ùå Error at row 1626, attempt 10: No JSON block found\n",
      "‚ùå Error at row 1626, attempt 11: No JSON block found\n",
      "‚ùå Error at row 1626, attempt 12: Expecting value: line 4 column 32 (char 163)\n",
      "‚ùå Error at row 1626, attempt 13: Expecting value: line 4 column 28 (char 155)\n",
      "‚ùå Error at row 1626, attempt 14: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1805/1805 [05:07<00:00,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 1626, attempt 15: Expecting value: line 4 column 28 (char 155)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_file)\n",
    "llm_fields = [\n",
    "    \"llm_length_effort\", \"llm_lexical_diversity\", \"llm_questions_raised\",\n",
    "    \"llm_citation_usage\", \"llm_sentiment_polarity\", \"llm_politeness\", \"llm_hedging\",\n",
    "    \"llm_specificity\", \"llm_domain_terms\", \"llm_relevance_alignment\",\n",
    "    \"llm_readability\", \"llm_overall_quality\", \"llm_overall_score_100\"\n",
    "]\n",
    "\n",
    "# Check for missing fields and add them if not present\n",
    "for field in llm_fields:\n",
    "    if field not in df.columns:\n",
    "        df[field] = pd.NA\n",
    "\n",
    "# Pattern to extract JSON block\n",
    "pattern = re.compile(r\"<review_assessment>\\s*(\\{.*?\\})\\s*</review_assessment>\", re.DOTALL)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"# REVIEW-QUALITY JUDGE\n",
    "\n",
    "## 0 ‚Äî ROLE  \n",
    "You are **ReviewInspector-LLM**, a rigorous, impartial meta-reviewer.  \n",
    "Your goal is to grade the quality of a single peer-review against a rich set of predefined criteria and to provide concise, actionable feedback.\n",
    "\n",
    "## 1 ‚Äî INPUTS  \n",
    "Title : {title}  \n",
    "Abstract : {abstract}  \n",
    "Review: {review_text}\n",
    "\n",
    "## 2 ‚Äî EVALUATION CRITERIA  \n",
    "Return **only** the scale value or label at right (no rationale text).\n",
    "\n",
    "| # | Criterion | Allowed scale / label |\n",
    "|---|-----------|-----------------------|\n",
    "| 1 | **Length & Effort**             | integer **0-5** |\n",
    "| 2 | **Lexical Diversity (TTR)**     | integer **0-5** |\n",
    "| 3 | **Number of Questions Raised**  | non-negative **integer** |\n",
    "| 4 | **Citation Usage**              | **yes / no** |\n",
    "| 5 | **Sentiment Polarity**          | **negative / neutral / positive** |\n",
    "| 6 | **Politeness**                  | **polite / neutral / impolite** |\n",
    "| 7 | **Hedging / Uncertainty**       | **No Hedging / Minimal / Moderate / Heavy / Extreme** |\n",
    "| 8 | **Specificity**                 | **very specific / somewhat specific / neutral / broad / very broad** |\n",
    "| 9 | **Use of Domain-Specific Terms**| integer **0-5** |\n",
    "|10 | **Relevance Alignment**         | integer **0-5** |\n",
    "|11 | **Readability**                 | integer **0-5** |\n",
    "|12 | **Overall Quality**             | integer **0-100** |\n",
    "\n",
    "## 3 ‚Äî SCORING GUIDELINES  \n",
    "For 0-5 scales: 5 = Outstanding, 4 = Strong, 3 = Adequate, 2 = Weak, 1 = Very weak, 0 = Absent/irrelevant.\n",
    "\n",
    "## 4 ‚Äî ANALYSIS & COMPUTATION (silent)  \n",
    "1. Read and comprehend the review text.  \n",
    "2. Compute raw metrics (word count, TTR, sentiment, FK grade, counts, etc.) or otherwise quantify qualitative aspects.  \n",
    "3. Map raw metrics to the scales above.\n",
    "\n",
    "## 5 ‚Äî OUTPUT FORMAT (strict)  \n",
    "Return **exactly one** JSON block wrapped in the tag below ‚Äî **no comments or extra text**.\n",
    "\n",
    "```json\n",
    "<review_assessment>\n",
    "{{\n",
    "  \"paper_title\": \"{title}\",\n",
    "  \"criteria\": {{\n",
    "    \"length_effort\":       ...,\n",
    "    \"lexical_diversity\":   ...,\n",
    "    \"questions_raised\":    ...,\n",
    "    \"citation_usage\":      ...,\n",
    "    \"sentiment_polarity\":  ...,\n",
    "    \"politeness\":          ...,\n",
    "    \"hedging\":             ...,\n",
    "    \"specificity\":         ...,\n",
    "    \"domain_terms\":        ...,\n",
    "    \"relevance_alignment\": ...,\n",
    "    \"readability\":         ...,\n",
    "    \"overall_quality\":     ...\n",
    "  }},\n",
    "  \"overall_score_100\": ...\n",
    "}}\n",
    "</review_assessment>\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring with LLM\"):\n",
    "    # Skip if all llm fields are already filled\n",
    "    if all(pd.notna(row.get(field, pd.NA)) for field in llm_fields):\n",
    "        continue\n",
    "\n",
    "    prompt = template.format(\n",
    "        title=row['title'],\n",
    "        abstract=row['abstract'],\n",
    "        review_text=row['review_text']\n",
    "    )\n",
    "\n",
    "    for attempt in range(15):\n",
    "        try:\n",
    "            response = chat(\"llama3.2\", messages=[{'role': 'user', 'content': prompt}])\n",
    "            content = response['message']['content']\n",
    "            match = pattern.search(content)\n",
    "            if not match:\n",
    "                raise ValueError(\"No JSON block found\")\n",
    "\n",
    "            parsed = json.loads(match.group(1))\n",
    "            for key, val in parsed[\"criteria\"].items():\n",
    "                df.at[idx, f\"llm_{key}\"] = val\n",
    "            df.at[idx, \"llm_overall_score_100\"] = parsed[\"overall_score_100\"]\n",
    "\n",
    "            # Save after every successful row\n",
    "            df.to_csv(input_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}, attempt {attempt + 1}: {e}\")\n",
    "            # time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùó Incomplete rows: 11 out of 1805\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Define the expected LLM columns (excluding 'paper_title')\n",
    "llm_columns = [\n",
    "    \"llm_length_effort\",\n",
    "    \"llm_lexical_diversity\",\n",
    "    \"llm_questions_raised\",\n",
    "    \"llm_citation_usage\",\n",
    "    \"llm_sentiment_polarity\",\n",
    "    \"llm_politeness\",\n",
    "    \"llm_hedging\",\n",
    "    \"llm_specificity\",\n",
    "    \"llm_domain_terms\",\n",
    "    \"llm_relevance_alignment\",\n",
    "    \"llm_readability\",\n",
    "    \"llm_overall_quality\",\n",
    "    \"llm_overall_score_100\"\n",
    "]\n",
    "\n",
    "# Find rows with any missing LLM fields\n",
    "incomplete_rows = df[df[llm_columns].isnull().any(axis=1)]\n",
    "\n",
    "# Print count\n",
    "print(f\"‚ùó Incomplete rows: {len(incomplete_rows)} out of {len(df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
