{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openreview-py numpy pandas nltk transformers tqdm torch\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install pylats taaled spacy convokit textstat simpletransformers\n",
    "\n",
    "%pip install spacy\n",
    "# English models\n",
    "%python -m spacy download en_core_web_sm\n",
    "%python -m spacy download en_core_web_trf\n",
    "# Spanish models (used as fallback)\n",
    "%python -m spacy download es_core_news_sm\n",
    "%python -m spacy download es_dep_news_trf\n",
    "\n",
    "%pip install textblob\n",
    "%python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "from convokit import Corpus, TextParser, PolitenessStrategies, Classifier, Utterance, Speaker, download\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURIPS 2023\n",
    "file_path = '../data/raw/neurips-2023.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.pkl' with the path to your .pkl file\n",
    "output_json_path = '../data/processed/neurips-2023.json'\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(file_path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "\n",
    "# Extract the required fields for each submission\n",
    "extracted_data = []\n",
    "for submission in data:\n",
    "    extracted_data.append({\n",
    "        'number': submission.number if hasattr(submission, 'number') else np.nan,\n",
    "        'id': submission.id if hasattr(submission, 'id') else np.nan,\n",
    "        'content.paperhash': submission.content['paperhash']['value'] if 'paperhash' in submission.content and 'value' in submission.content['paperhash'] else np.nan,\n",
    "        'content.authorids': submission.content['authorids']['value'] if 'authorids' in submission.content and 'value' in submission.content['authorids'] else np.nan,\n",
    "        'cdate': submission.cdate if hasattr(submission, 'cdate') else np.nan,\n",
    "        'content.title': submission.content['title']['value'] if 'title' in submission.content and 'value' in submission.content['title'] else np.nan,\n",
    "        'content.abstract': submission.content['abstract']['value'] if 'abstract' in submission.content and 'value' in submission.content['abstract'] else np.nan,\n",
    "        # 'content.TLDR': submission.content['TLDR']['value'] if 'TLDR' in submission.content and 'value' in submission.content['TLDR'] else np.nan,\n",
    "    })\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(output_json_path, orient='records', indent=4)\n",
    "print(f\"DataFrame saved to JSON file at: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file as a pandas DataFrame\n",
    "df_json = pd.read_json('../data/processed/neurips-2023.json')\n",
    "\n",
    "# Display the header of the first 5 samples\n",
    "df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of review, comments, and meta-reviews for each submission\n",
    "# 3395 is the number of submissions\n",
    "num_of_reviews = [len(data[i].details['directReplies']) for i in range(3395)]\n",
    "print('min:', min(num_of_reviews))\n",
    "print('max:', max(num_of_reviews))\n",
    "print('mean:', np.mean(num_of_reviews))\n",
    "print('median:', np.median(num_of_reviews))\n",
    "print('std:', np.std(num_of_reviews))\n",
    "print('percentiles:', np.percentile(num_of_reviews, [25, 50, 75, 90, 95, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_types = set()\n",
    "for i in range(3395):\n",
    "    for j in range(len(data[i].details['directReplies'])):\n",
    "        comment_types.add(data[i].details['directReplies'][j]['invitations'][0].split('/')[-1])\n",
    "\n",
    "comment_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each submission in the data\n",
    "new_rows = []\n",
    "for submission in data:\n",
    "    submission_id = submission.id\n",
    "    submission_number = submission.number\n",
    "    submission_title = submission.content['title']['value'] if 'title' in submission.content and 'value' in submission.content['title'] else None\n",
    "    submission_abstract = submission.content['abstract']['value'] if 'abstract' in submission.content and 'value' in submission.content['abstract'] else None\n",
    "    submission_authors = submission.content['authorids']['value'] if 'authorids' in submission.content and 'value' in submission.content['authorids'] else np.nan\n",
    "    submission_creation_date = submission.cdate if hasattr(submission, 'cdate') else np.nan\n",
    "\n",
    "    # Check if 'directReplies' exists in details\n",
    "    if 'directReplies' in submission.details:\n",
    "        for reply in submission.details['directReplies']:\n",
    "            # Check if the invitation is 'Official_Review'\n",
    "            if reply['invitations'][0].split('/')[-1] == 'Official_Review':\n",
    "                # Extract features from the review\n",
    "                reviewer = reply['signatures'][0].split('/')[-1]  # if 'signatures' in reply and len(reply['signatures']) > 0 else None\n",
    "                \n",
    "                #'tcdate', 'cdate', 'tmdate', 'mdate'\n",
    "                review_tcdate = reply['tcdate'] if 'tcdate' in reply else None\n",
    "                review_cdate = reply['cdate'] if 'cdate' in reply else None\n",
    "                review_tmdate = reply['tmdate'] if 'tmdate' in reply else None\n",
    "                review_mdate = reply['mdate'] if 'mdate' in reply else None\n",
    "                \n",
    "                review_rating = int(reply['content']['rating']['value'].split(':')[0]) if 'rating' in reply['content'] and 'value' in reply['content']['rating'] else None\n",
    "                review_confidence = int(reply['content']['confidence']['value'].split(':')[0]) if 'confidence' in reply['content'] and 'value' in reply['content']['confidence'] else None\n",
    "                review_soundness = int(reply['content']['soundness']['value'].split(' ')[0]) if 'soundness' in reply['content'] and 'value' in reply['content']['soundness'] else None\n",
    "                review_presentation = int(reply['content']['presentation']['value'].split(' ')[0]) if 'presentation' in reply['content'] and 'value' in reply['content']['presentation'] else None\n",
    "                review_contribution = int(reply['content']['contribution']['value'].split(' ')[0]) if 'contribution' in reply['content'] and 'value' in reply['content']['contribution'] else None\n",
    "                \n",
    "                review_summary = reply['content']['summary']['value'] if 'summary' in reply['content'] and 'value' in reply['content']['summary'] else None\n",
    "                review_strengths = reply['content']['strengths']['value'] if 'strengths' in reply['content'] and 'value' in reply['content']['strengths'] else None\n",
    "                review_weaknesses = reply['content']['weaknesses']['value'] if 'weaknesses' in reply['content'] and 'value' in reply['content']['weaknesses'] else None\n",
    "                review_questions = reply['content']['questions']['value'] if 'questions' in reply['content'] and 'value' in reply['content']['questions'] else None\n",
    "                review_limitations = reply['content']['limitations']['value'] if 'limitations' in reply['content'] and 'value' in reply['content']['limitations'] else None\n",
    "                \n",
    "                # Create a new row with the extracted features\n",
    "                new_row = {\n",
    "                    'submission_id': submission_id,\n",
    "                    'submission_number': submission_number,\n",
    "                    'submission_creation_date': submission_creation_date,\n",
    "                    'submission_authors': submission_authors,\n",
    "                    \n",
    "                    'submission_title': submission_title,\n",
    "                    'submission_abstract': submission_abstract,\n",
    "                    \n",
    "                    'reviewer': reviewer,\n",
    "                    'review_tcdate': review_tcdate,\n",
    "                    'review_cdate': review_cdate,\n",
    "                    'review_tmdate': review_tmdate,\n",
    "                    'review_mdate': review_mdate,\n",
    "                    \n",
    "                    'review_summary': review_summary,\n",
    "                    'review_strengths': review_strengths,\n",
    "                    'review_weaknesses': review_weaknesses,\n",
    "                    'review_questions': review_questions,\n",
    "                    'review_limitations': review_limitations,\n",
    "                    \n",
    "                    'review_rating': review_rating,\n",
    "                    'review_confidence': review_confidence,\n",
    "                    'review_soundness': review_soundness,\n",
    "                    'review_presentation': review_presentation,\n",
    "                    'review_contribution': review_contribution\n",
    "                }\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame from the new rows\n",
    "df_reviews = pd.DataFrame(new_rows)\n",
    "\n",
    "# Save the updated DataFrame to a JSON file\n",
    "output_json_path = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(output_json_path, orient='records', indent=4)\n",
    "\n",
    "print(f\"Updated DataFrame with reviews saved to JSON file at: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('../data/processed/neurips-2023.json')\n",
    "\n",
    "# Display the header of the first 5 samples\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the specified columns into a single column named 'total_review'\n",
    "df_reviews['total_review'] = df_reviews[['review_summary', 'review_strengths', 'review_weaknesses', 'review_questions', 'review_limitations']].apply(\n",
    "    lambda row: ' '.join(row.dropna()), axis=1\n",
    ")\n",
    "\n",
    "# Drop the original columns to reduce redundancy\n",
    "df_reviews = df_reviews.drop(columns=['review_summary', 'review_strengths', 'review_weaknesses', 'review_questions', 'review_limitations'])\n",
    "# Create a new column 'length_words' to count the number of words in the 'total_review' column\n",
    "df_reviews['length_words'] = df_reviews['total_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "# Save the new DataFrame to a different JSON file\n",
    "new_output_json_path = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_total_review.json'\n",
    "df_reviews.to_json(new_output_json_path, orient='records', indent=4)\n",
    "\n",
    "print(f\"New DataFrame with 'total_review' column saved to JSON file at: {new_output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "# Apply the count_citations function to the 'total_review' column and create a new column 'citation_count'\n",
    "df_reviews['citation_count'] = df_reviews['total_review'].apply(count_citations)\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def count_questions(review_text):\n",
    "    \n",
    "    if review_text:\n",
    "        question_count = 0\n",
    "\n",
    "        sentences = sent_tokenize(review_text)\n",
    "        for sent in sentences:\n",
    "            inputs = tokenizer(\n",
    "                sent,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=64,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                # Label 0 = question\n",
    "                if predicted == 0:\n",
    "                    question_count += 1\n",
    "\n",
    "    return question_count\n",
    "\n",
    "\n",
    "df_reviews['question_count'] = [\n",
    "    count_questions(row['total_review']) for row in tqdm(df_reviews.to_dict('records'), desc=\"Processing reviews\")\n",
    "]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows where cdate and tcdate are equal or not equal\n",
    "cdate_tcdate_equal = (df_reviews['review_cdate'] == df_reviews['review_tcdate']).sum()\n",
    "cdate_tcdate_not_equal = (df_reviews['review_cdate'] != df_reviews['review_tcdate']).sum()\n",
    "\n",
    "# Count rows where tmdate and mdate are equal or not equal\n",
    "tmdate_mdate_equal = (df_reviews['review_tmdate'] == df_reviews['review_mdate']).sum()\n",
    "tmdate_mdate_not_equal = (df_reviews['review_tmdate'] != df_reviews['review_mdate']).sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"cdate and tcdate equal: {cdate_tcdate_equal}\")\n",
    "print(f\"cdate and tcdate not equal: {cdate_tcdate_not_equal}\")\n",
    "print(f\"tmdate and mdate equal: {tmdate_mdate_equal}\")\n",
    "print(f\"tmdate and mdate not equal: {tmdate_mdate_not_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'tcdate' and 'tmdate' columns\n",
    "df_reviews = df_reviews.drop(columns=['review_tcdate', 'review_tmdate'])\n",
    "\n",
    "# Rename 'cdate' to 'creation_date' and 'mdate' to 'last_modification_date'\n",
    "df_reviews = df_reviews.rename(columns={'review_cdate': 'creation_date', 'review_mdate': 'last_modification_date'})\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('../data/processed/neurips-2023.json')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mattr(review_text):\n",
    "    mattr_value = \"\"\n",
    "    if review_text is not None:\n",
    "        review_text = review_text.strip()  # Remove leading/trailing whitespace\n",
    "        review_text = review_text.replace('\\n', '')  # Replace newlines with spaces\n",
    "        try:\n",
    "            cleaned = lats.Normalize(review_text, lats.ld_params_en)\n",
    "            tokens = cleaned.toks\n",
    "            mattr_value = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "        except Exception as e:\n",
    "            mattr_value = \"\"\n",
    "    return mattr_value\n",
    "\n",
    "\n",
    "df_reviews['mattr'] = [\n",
    "    compute_mattr(row['total_review']) for row in tqdm(df_reviews.to_dict('records'), desc=\"Processing reviews\")\n",
    "]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_polarity(review_text):\n",
    "    review_text = review_text.strip()\n",
    "    try:\n",
    "        blob = TextBlob(review_text)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "    except Exception:\n",
    "        sentiment = \"\"\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "df_reviews['sentiment_polarity'] = [\n",
    "    compute_sentiment_polarity(row['total_review']) for row in tqdm(df_reviews.to_dict('records'), desc=\"Processing reviews\")\n",
    "]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('../data/processed/neurips-2023.json')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load SPECTER model ---\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def encoding_text(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compute_relevance_score(review_text, title, abstract):\n",
    "    # Encode document\n",
    "    doc_emb = encoding_text(f\"{title} {abstract}\")\n",
    "\n",
    "    # Encode review text\n",
    "    review_emb = encoding_text(review_text)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    return F.cosine_similarity(doc_emb, review_emb).item()\n",
    "\n",
    "\n",
    "# Compute similarity score for each row with progress bar\n",
    "df_reviews['similarity_score'] = [\n",
    "    compute_relevance_score(row['total_review'], row['submission_title'], row['submission_abstract'])\n",
    "    for row in tqdm(df_reviews.to_dict('records'), desc=\"Computing similarity scores\")\n",
    "]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('../data/processed/neurips-2023.json')\n",
    "# Display the first few rows of the DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the time differences and add new columns\n",
    "df_reviews['paper_submission_to_review_submission_time'] = df_reviews['last_modification_date'] - df_reviews['submission_creation_date']\n",
    "df_reviews['review_creation_to_review_submission_time'] = df_reviews['last_modification_date'] - df_reviews['creation_date']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Convert time from milliseconds to days\n",
    "df_reviews['paper_submission_to_review_submission_days'] = df_reviews['paper_submission_to_review_submission_time'] // (24 * 60 * 60 * 1000)\n",
    "df_reviews['review_creation_to_review_submission_days'] = df_reviews['review_creation_to_review_submission_time'] // (24 * 60 * 60 * 1000)\n",
    "\n",
    "# Drop the original columns to avoid redundancy\n",
    "df_reviews = df_reviews.drop(columns=['paper_submission_to_review_submission_time', 'review_creation_to_review_submission_time'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('../data/processed/neurips-2023.json')\n",
    "# Display the first few rows of the DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Scoring Readability\")\n",
    "\n",
    "# Define the readability scoring function\n",
    "def readability_scores(text):\n",
    "    try:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": round(textstat.flesch_reading_ease(text), 4),\n",
    "            \"flesch_kincaid_grade\": round(textstat.flesch_kincaid_grade(text), 4),\n",
    "            \"gunning_fog\": round(textstat.gunning_fog(text), 4),\n",
    "            \"smog_index\": round(textstat.smog_index(text), 4),\n",
    "            \"automated_readability_index\": round(textstat.automated_readability_index(text), 4),\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": None,\n",
    "            \"flesch_kincaid_grade\": None,\n",
    "            \"gunning_fog\": None,\n",
    "            \"smog_index\": None,\n",
    "            \"automated_readability_index\": None,\n",
    "        }\n",
    "\n",
    "readability_scores_df = df_reviews['total_review'].progress_apply(readability_scores).apply(pd.Series)\n",
    "df_reviews = pd.concat([df_reviews, readability_scores_df], axis=1)\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Load training corpus\n",
    "print(\"üì• Downloading training corpus...\")\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "\n",
    "# Step 2: Convert review data to Utterances with dummy speakers\n",
    "review_utterances = []\n",
    "for idx, row in tqdm(df_reviews.iterrows(), desc=\"üîß Preparing Utterances\", total=len(df_reviews)):\n",
    "    review_text = row['total_review'].strip()\n",
    "    if review_text:\n",
    "        dummy_speaker = Speaker(id=f\"reviewer_{idx}\")\n",
    "        review_utterances.append(\n",
    "            Utterance(id=str(idx), text=review_text, speaker=dummy_speaker, meta={\"orig_row\": row})\n",
    "        )\n",
    "\n",
    "# Step 3: Build test corpus\n",
    "print(\"üì¶ Building test corpus...\")\n",
    "test_corpus = Corpus(utterances=review_utterances)\n",
    "\n",
    "# Step 4: Parse\n",
    "print(\"üß† Parsing utterances...\")\n",
    "parser = TextParser()\n",
    "parser.transform(train_corpus)\n",
    "parser.transform(test_corpus)\n",
    "\n",
    "# Step 5: Extract politeness strategies\n",
    "print(\"‚ú® Extracting politeness strategies...\")\n",
    "ps = PolitenessStrategies()\n",
    "ps.transform(train_corpus)\n",
    "ps.transform(test_corpus)\n",
    "\n",
    "# Step 6: Train classifier\n",
    "print(\"üéì Training classifier...\")\n",
    "clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'],\n",
    "                 labeller=lambda utt: utt.meta.get(\"Binary\") == 1)\n",
    "\n",
    "# Move classifier to GPU\n",
    "clf.device = device\n",
    "\n",
    "# Train on GPU\n",
    "clf.fit(train_corpus)\n",
    "\n",
    "# Test on GPU\n",
    "clf.transform(test_corpus)\n",
    "\n",
    "# Step 7: Compute politeness scores and add them to the dataframe\n",
    "print(\"üìà Computing politeness scores...\")\n",
    "politeness_scores = []\n",
    "for utt in tqdm(test_corpus.iter_utterances(), desc=\"üîó Assigning Scores\"):\n",
    "    try:\n",
    "        score = clf.summarize(test_corpus).loc[utt.id, \"pred_score\"]\n",
    "        politeness_scores.append(round(score, 4))\n",
    "    except KeyError:\n",
    "        politeness_scores.append(\"\")\n",
    "\n",
    "# Add politeness scores to the dataframe\n",
    "df_reviews['politeness_score'] = politeness_scores\n",
    "\n",
    "print(\"‚úÖ Politeness scores added to the dataframe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('../data/processed/neurips-2023.json')\n",
    "# Display the first few rows of the DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels used by the HEDGEhog model\n",
    "labels = [\"C\", \"D\", \"E\", \"I\", \"N\"]\n",
    "\n",
    "# Set up model arguments\n",
    "model_args = NERArgs()\n",
    "model_args.labels_list = labels\n",
    "model_args.silent = True\n",
    "model_args.use_multiprocessing = False\n",
    "\n",
    "# Initialize model\n",
    "model = NERModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"jeniakim/hedgehog\",\n",
    "    args=model_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Function to count each label type\n",
    "def count_hedge_labels(text):\n",
    "    predictions, _ = model.predict([text])\n",
    "    token_labels = [list(token.values())[0] for token in predictions[0]]\n",
    "    counts = Counter(token_labels)\n",
    "    return {label: counts.get(label, 0) for label in labels}\n",
    "\n",
    "\n",
    "# Escape brackets in the 'total_review' column; brackets [] are raising errors for hedge function. I should add \\ before them.\n",
    "df_reviews['total_review'] = df_reviews['total_review'].apply(\n",
    "    lambda x: re.sub(r'([\\[\\]])', r'\\\\\\1', x)\n",
    ")\n",
    "\n",
    "# Apply count_hedge_labels to the 'total_review' column\n",
    "tqdm.pandas(desc=\"Counting Hedge Labels\")\n",
    "hedge_counts = df_reviews[\"total_review\"].progress_apply(count_hedge_labels)\n",
    "\n",
    "# Convert the dictionary output into separate columns\n",
    "for label in labels:\n",
    "    df_reviews[f\"hedge_{label}\"] = hedge_counts.apply(lambda x: x.get(label, 0))\n",
    "\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '../data/processed/neurips-2023.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/processed/neurips-2023.json'\n",
    "\n",
    "df = pd.read_json(input_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 1000 unique submission_number values\n",
    "selected_submission_numbers = random.sample(df['submission_number'].unique().tolist(), 1000)\n",
    "\n",
    "# Filter rows with the selected submission_number values\n",
    "df_1000 = df[df['submission_number'].isin(selected_submission_numbers)]\n",
    "\n",
    "# Display the new dataframe\n",
    "df_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000.to_json('../data/processed/neurips-2023-1000-papers.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/processed/neurips-2023-1000-papers.json'\n",
    "# Load data\n",
    "df = pd.read_json(input_file)\n",
    "\n",
    "llm_fields = [\n",
    "    \"llm_Comprehensiveness\", \"llm_Vagueness\", \"llm_Objectivity\", \"llm_Fairness\", \"llm_Actionability\", \n",
    "    \"llm_Constructiveness\", \"llm_Relevance Alignment\", \"llm_Clarity and Readability\", \"llm_Usage of Technical Terms\",\n",
    "    \"llm_Factuality\", \"llm_Overall Quality\", \"llm_overall_score_100\", \"llm_Sentiment Polarity\", \"llm_Politeness\", \n",
    "]\n",
    "\n",
    "\n",
    "# Check for missing fields and add them if not present\n",
    "for field in llm_fields:\n",
    "    if field not in df.columns:\n",
    "        df[field] = pd.NA\n",
    "\n",
    "# Pattern to extract JSON block\n",
    "pattern = re.compile(r\"<review_assessment>\\s*(\\{.*?\\})\\s*</review_assessment>\", re.DOTALL)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"# REVIEW-QUALITY JUDGE\n",
    "\n",
    "## 0 ‚Äî ROLE\n",
    "\n",
    "You are **ReviewInspector-LLM**, a rigorous, impartial meta-reviewer.\n",
    "Your goal is to assess the quality of a single peer-review against a predefined set of criteria and to provide precise, structured evaluations.\n",
    "\n",
    "## 1 ‚Äî INPUTS\n",
    "\n",
    "Title: {title}\n",
    "Abstract: {abstract}\n",
    "Review: {review_text}\n",
    "\n",
    "## 2 ‚Äî EVALUATION CRITERIA\n",
    "\n",
    "Return **only** the scale value or label at right (no rationale text).\n",
    "\n",
    "| #  | Criterion                    | Allowed scale / label                       | Description                                                                |\n",
    "| -- | ---------------------------- | ------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| 1  | **Comprehensiveness**        | integer **0-5**                             | Extent to which the review covers all key aspects of the paper.            |\n",
    "| 2  | **Usage of Technical Terms** | integer **0-5**                             | Appropriateness and frequency of domain-specific vocabulary.               |\n",
    "| 3  | **Factuality**               | **factual / partially factual / unfactual** | Accuracy of the statements made in the review.                             |\n",
    "| 4  | **Sentiment Polarity**       | **negative / neutral / positive**           | Overall sentiment conveyed by the reviewer.                                |\n",
    "| 5  | **Politeness**               | **polite / neutral / impolite**             | Tone and manner of the review language.                                    |\n",
    "| 6  | **Vagueness**                | **none / low / moderate / high / extreme**  | Degree of ambiguity or lack of specificity in the review.                  |\n",
    "| 7  | **Objectivity**              | integer **0-5**                             | Presence of unbiased, evidence-based commentary.                           |\n",
    "| 8  | **Fairness**                 | integer **0-5**                             | Perceived impartiality and balance in judgments.                           |\n",
    "| 9  | **Actionability**            | integer **0-5**                             | Helpfulness of the review in suggesting clear next steps.                  |\n",
    "| 10 | **Constructiveness**         | integer **0-5**                             | Degree to which the review offers improvements rather than just criticism. |\n",
    "| 11 | **Relevance Alignment**      | integer **0-5**                             | How well the review relates to the content and scope of the paper.         |\n",
    "| 12 | **Clarity and Readability**  | integer **0-5**                             | Ease of understanding the review, including grammar and structure.         |\n",
    "| 13 | **Overall Quality**          | integer **0-100**                           | Holistic evaluation of the review's usefulness and professionalism.        |\n",
    "\n",
    "## 3 ‚Äî SCORING GUIDELINES\n",
    "\n",
    "For 0-5 scales:\n",
    "\n",
    "* 5 = Outstanding\n",
    "* 4 = Strong\n",
    "* 3 = Adequate\n",
    "* 2 = Weak\n",
    "* 1 = Very weak\n",
    "* 0 = Absent/irrelevant\n",
    "\n",
    "## 4 ‚Äî ANALYSIS & COMPUTATION (silent)\n",
    "\n",
    "1. Read and understand the review in the context of the paper title and abstract.\n",
    "2. Extract quantitative and qualitative signals (e.g., term usage, factual consistency, tone, clarity).\n",
    "3. Map observations to the corresponding scoring scales.\n",
    "\n",
    "## 5 ‚Äî OUTPUT FORMAT (strict)  \n",
    "Return **exactly one** JSON block wrapped in the tag below ‚Äî **no comments or extra text**.\n",
    "\n",
    "```json\n",
    "<review_assessment>\n",
    "{{\n",
    "  \"paper_title\": \"{title}\",\n",
    "  \"criteria\": {{\n",
    "    \"Comprehensiveness\":       ...,\n",
    "    \"Usage of Technical Terms\":   ...,\n",
    "    \"Factuality\":    ...,\n",
    "    \"Sentiment Polarity\":      ...,\n",
    "    \"Politeness\":  ...,\n",
    "    \"Vagueness\":          ...,\n",
    "    \"Objectivity\":             ...,\n",
    "    \"Fairness\":         ...,\n",
    "    \"Actionability\":        ...,\n",
    "    \"Constructiveness\":    ...,\n",
    "    \"Relevance Alignment\":    ...,\n",
    "    \"Clarity and Readability\":    ...,\n",
    "    \"Relevance Alignment\":    ...,\n",
    "    \"Overall Quality\":     ...\n",
    "  }},\n",
    "  \"overall_score_100\": ...\n",
    "}}\n",
    "</review_assessment>\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the temperature parameter for the llama model\n",
    "temperature = 0\n",
    "seed = 42\n",
    "llm_name = \"qwen3:8b\"  # llama3:8b, qwen3:8b\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring with LLM\"):\n",
    "    # Skip if all llm fields are already filled\n",
    "    if all(pd.notna(row.get(field, pd.NA)) for field in llm_fields):\n",
    "        continue\n",
    "\n",
    "    prompt = template.format(\n",
    "        title=row['submission_title'],\n",
    "        abstract=row['submission_abstract'],\n",
    "        review_text=row['total_review']\n",
    "    )\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = chat(llm_name, messages=[{'role': 'user', 'content': prompt}], options={'temperature': temperature, 'seed': seed})\n",
    "            content = response['message']['content']\n",
    "            match = pattern.search(content)\n",
    "            if not match:\n",
    "                raise ValueError(\"No JSON block found\")\n",
    "\n",
    "            parsed = json.loads(match.group(1))\n",
    "            print(parsed[\"overall_score_100\"])\n",
    "            for key, val in parsed[\"criteria\"].items():\n",
    "                df.at[idx, f\"llm_{key}\"] = val\n",
    "            df.at[idx, \"llm_overall_score_100\"] = parsed[\"overall_score_100\"]\n",
    "\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}, attempt {attempt + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(f'../data/processed/neurips-2023-1000-{llm_name}.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
