{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "with open('../data/raw/f1000research.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/processed/f1000research.csv\"\n",
    "input_file = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "rows = []\n",
    "\n",
    "for entry in data:\n",
    "    if len(entry) != 1:\n",
    "        continue  # skip multi-version papers\n",
    "\n",
    "    paper_obj = entry[0]\n",
    "    paper_info = paper_obj.get(\"paper\", {})\n",
    "    reviews = paper_obj.get(\"reviews\", [])\n",
    "    date_str = paper_obj.get(\"date\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        paper_date = datetime.strptime(date_str, \"%d %b %y\")\n",
    "    except Exception:\n",
    "        paper_date = None\n",
    "\n",
    "    title = paper_info.get(\"title\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    abstract = paper_info.get(\"abstract\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "    for review in reviews:\n",
    "        reviewer = review.get(\"name\", \"Anonymous\").strip()\n",
    "        review_date_str = review.get(\"date\", \"\").strip()\n",
    "\n",
    "        review_text = review.get(\"report\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "        review_suggestion = review.get(\"suggestion\", \"\").strip()\n",
    "        length_words = len(review_text.split())\n",
    "\n",
    "        try:\n",
    "            review_date = datetime.strptime(review_date_str, \"%d %b %Y\")\n",
    "            days_to_submit = (review_date - paper_date).days if paper_date else None\n",
    "        except Exception:\n",
    "            days_to_submit = None\n",
    "\n",
    "        rows.append({\n",
    "            \"reviewer\": reviewer,\n",
    "            \"review_date\": review_date_str,\n",
    "            \"review_suggestion\": review_suggestion,\n",
    "            \"length_words\": length_words,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"days_to_submit\": days_to_submit,\n",
    "            \"review_text\": review_text,\n",
    "        })\n",
    "\n",
    "# Save the cleaned reviews to a new CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"‚úÖ Cleaned and saved {len(rows)} reviews to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install taaled pylats spacy\n",
    "# English models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "\n",
    "# Spanish models (used as fallback)\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download es_dep_news_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# --- Load and prepare ---\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "if \"mattr\" not in fieldnames:\n",
    "    fieldnames.append(\"mattr\")\n",
    "if \"mattr_reason\" in fieldnames:\n",
    "    fieldnames.remove(\"mattr_reason\")\n",
    "\n",
    "params = lats.ld_params_en  # Cache once\n",
    "\n",
    "def compute_mattr(row):\n",
    "    review_text = row.get(\"review_text\", \"\").strip()\n",
    "    try:\n",
    "        cleaned = lats.Normalize(review_text, params)\n",
    "        tokens = cleaned.toks\n",
    "        row[\"mattr\"] = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "    except Exception:\n",
    "        row[\"mattr\"] = \"\"\n",
    "    row.pop(\"mattr_reason\", None)\n",
    "    return row\n",
    "\n",
    "# --- Parallel execution ---\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    output_rows = list(tqdm(executor.map(compute_mattr, reader), total=len(reader), desc=\"Parallel MATTR\"))\n",
    "\n",
    "# --- Save back to CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Parallel MATTR values saved to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk\n",
    "\n",
    "###########################\n",
    "# Apple silicon support\n",
    "# Uninstall current PyTorch version (if any)\n",
    "# !pip uninstall torch -y\n",
    "\n",
    "# Install PyTorch with MPS (Metal Performance Shaders) support\n",
    "# !pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "###########################\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK punkt tokenizer is available\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Load review rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "    if \"question_count\" not in fieldnames:\n",
    "        fieldnames.append(\"question_count\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Detecting Questions\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        question_count = 0\n",
    "\n",
    "        try:\n",
    "            sentences = sent_tokenize(review_text)\n",
    "            for sent in sentences:\n",
    "                inputs = tokenizer(\n",
    "                    sent,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=64,\n",
    "                    padding=True\n",
    "                ).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                    # Label 0 = question\n",
    "                    if predicted == 0:\n",
    "                        question_count += 1\n",
    "        except Exception as e:\n",
    "            question_count = \"\"\n",
    "\n",
    "        row[\"question_count\"] = question_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Questions counted and saved in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Citation counting logic ---\n",
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "# --- Load CSV and apply ---\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Update for citation_count\n",
    "    if \"citation_count\" not in fieldnames:\n",
    "        fieldnames.append(\"citation_count\")\n",
    "    if \"has_citation\" in fieldnames:\n",
    "        fieldnames.remove(\"has_citation\")  # Remove old 'has_citation' if needed\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Counting Citations\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        citation_count = count_citations(review_text)\n",
    "        row[\"citation_count\"] = citation_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Citation counts added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(output_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    total = 0\n",
    "    with_citations = 0\n",
    "\n",
    "    for row in reader:\n",
    "        total += 1\n",
    "        if row.get(\"citation_count\") == \"0\":\n",
    "            with_citations += 1\n",
    "\n",
    "print(f\"üìÑ Total reviews: {total}\")\n",
    "print(f\"üîç Reviews with citations: {with_citations}\")\n",
    "print(f\"üìä Percentage: {(with_citations / total * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read and process the file\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Add new column if not already there\n",
    "    if \"sentiment_polarity\" not in fieldnames:\n",
    "        fieldnames.append(\"sentiment_polarity\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Analyzing Sentiment\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        try:\n",
    "            blob = TextBlob(review_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "        except Exception:\n",
    "            sentiment = \"\"\n",
    "\n",
    "        row[\"sentiment_polarity\"] = sentiment\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Sentiment polarity added to review_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install convokit\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, download, TextParser, PolitenessStrategies, Classifier, Utterance, Speaker\n",
    "\n",
    "# Step 1: Load training corpus\n",
    "print(\"üì• Downloading training corpus...\")\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "\n",
    "# Step 2: Load review data and convert to Utterances with dummy speakers\n",
    "review_utterances = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    for idx, row in tqdm(enumerate(reader), desc=\"üîß Preparing Utterances\", total=1805):  # Adjust total if needed\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        if review_text:\n",
    "            dummy_speaker = Speaker(id=f\"reviewer_{idx}\")\n",
    "            review_utterances.append(\n",
    "                Utterance(id=str(idx), text=review_text, speaker=dummy_speaker, meta={\"orig_row\": row})\n",
    "            )\n",
    "\n",
    "# Step 3: Build test corpus\n",
    "print(\"üì¶ Building test corpus...\")\n",
    "test_corpus = Corpus(utterances=review_utterances)\n",
    "\n",
    "# Step 4: Parse\n",
    "print(\"üß† Parsing utterances...\")\n",
    "parser = TextParser()\n",
    "parser.transform(train_corpus)\n",
    "parser.transform(test_corpus)\n",
    "\n",
    "# Step 5: Extract politeness strategies\n",
    "print(\"‚ú® Extracting politeness strategies...\")\n",
    "ps = PolitenessStrategies()\n",
    "ps.transform(train_corpus)\n",
    "ps.transform(test_corpus)\n",
    "\n",
    "# Step 6: Train classifier\n",
    "print(\"üéì Training classifier...\")\n",
    "clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'],\n",
    "                 labeller=lambda utt: utt.meta.get(\"Binary\") == 1)\n",
    "clf.fit(train_corpus)\n",
    "clf.transform(test_corpus)\n",
    "\n",
    "# Step 7: Summarize results\n",
    "print(\"üìà Summarizing scores...\")\n",
    "results = clf.summarize(test_corpus)\n",
    "\n",
    "# Step 8: Merge back to CSV rows\n",
    "print(\"üßæ Merging scores into CSV...\")\n",
    "output_rows = []\n",
    "fieldnames = list(reader[0].keys())\n",
    "if \"politeness_score\" not in fieldnames:\n",
    "    fieldnames.append(\"politeness_score\")\n",
    "\n",
    "for utt in tqdm(test_corpus.iter_utterances(), desc=\"üîó Assigning Scores\"):\n",
    "    row = utt.meta[\"orig_row\"]\n",
    "    try:\n",
    "        score = results.loc[utt.id, \"pred_score\"]\n",
    "        row[\"politeness_score\"] = round(score, 4)\n",
    "    except KeyError:\n",
    "        row[\"politeness_score\"] = \"\"\n",
    "    output_rows.append(row)\n",
    "\n",
    "# Step 9: Save\n",
    "print(\"üíæ Saving to review_analysis.csv...\")\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ All done! Politeness scores are now in your CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load SPECTER model\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    if \"similarity_score\" not in fieldnames:\n",
    "        fieldnames.append(\"similarity_score\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing Relevance Score\"):\n",
    "        try:\n",
    "            review_text = row.get(\"review_text\", \"\").strip()\n",
    "            title = row.get(\"title\", \"\").strip()\n",
    "            abstract = row.get(\"abstract\", \"\").strip()\n",
    "            doc_text = f\"{title} {abstract}\"\n",
    "\n",
    "            # Encode document\n",
    "            doc_inputs = tokenizer(doc_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            doc_inputs = {k: v.to(device) for k, v in doc_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                doc_emb = model(**doc_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "            # Encode review\n",
    "            review_inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            review_inputs = {k: v.to(device) for k, v in review_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                review_emb = model(**review_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "            # Cosine similarity\n",
    "            similarity_score = F.cosine_similarity(doc_emb, review_emb).item()\n",
    "            row[\"similarity_score\"] = similarity_score\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"‚úÖ Relevance scores added using title + abstract in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Scoring Readability\")\n",
    "\n",
    "# Define the readability scoring function\n",
    "def readability_scores(text):\n",
    "    try:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
    "            \"gunning_fog\": textstat.gunning_fog(text),\n",
    "            \"smog_index\": textstat.smog_index(text),\n",
    "            \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": None,\n",
    "            \"flesch_kincaid_grade\": None,\n",
    "            \"gunning_fog\": None,\n",
    "            \"smog_index\": None,\n",
    "            \"automated_readability_index\": None,\n",
    "        }\n",
    "\n",
    "# Apply function with progress bar\n",
    "readability_results = df[\"review_text\"].progress_apply(readability_scores)\n",
    "readability_df = pd.DataFrame(readability_results.tolist())\n",
    "\n",
    "# Merge new columns\n",
    "df = pd.concat([df, readability_df], axis=1)\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved enriched file to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# Define labels used by the HEDGEhog model\n",
    "labels = [\"C\", \"D\", \"E\", \"I\", \"N\"]\n",
    "\n",
    "# Set up model arguments\n",
    "model_args = NERArgs()\n",
    "model_args.labels_list = labels\n",
    "model_args.silent = True\n",
    "model_args.use_multiprocessing = False\n",
    "\n",
    "# Initialize model\n",
    "model = NERModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"jeniakim/hedgehog\",\n",
    "    args=model_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to count each label type\n",
    "def count_hedge_labels(text):\n",
    "    predictions, _ = model.predict([text])\n",
    "    token_labels = [list(token.values())[0] for token in predictions[0]]\n",
    "    counts = Counter(token_labels)\n",
    "    return {label: counts.get(label, 0) for label in labels}\n",
    "\n",
    "# Apply across review_text\n",
    "tqdm.pandas(desc=\"Counting Hedge Labels\")\n",
    "hedge_counts = df[\"review_text\"].progress_apply(count_hedge_labels)\n",
    "\n",
    "# Convert counts into separate columns and join with df\n",
    "hedge_df = pd.DataFrame(hedge_counts.tolist())\n",
    "hedge_df.columns = [f\"hedge_{label}\" for label in hedge_df.columns]\n",
    "\n",
    "df = pd.concat([df.reset_index(drop=True), hedge_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"‚úÖ Hedge label counts saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9310, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9312/10174 [2:46:22<52:45,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9312, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9312, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9312, attempt 3: No JSON block found\n",
      "‚ùå Error at row 9312, attempt 4: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9313/10174 [2:46:28<1:02:34,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9312, attempt 5: No JSON block found\n",
      "‚ùå Error at row 9313, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9314/10174 [2:46:32<59:51,  4.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9314, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9314, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9320/10174 [2:46:55<47:51,  3.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9320, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9320, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9320, attempt 3: No JSON block found\n",
      "‚ùå Error at row 9320, attempt 4: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9321/10174 [2:47:01<57:59,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9320, attempt 5: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9332/10174 [2:47:37<50:24,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9332, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9334/10174 [2:47:43<45:35,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9334, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9334, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9336/10174 [2:47:50<47:21,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9336, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9336, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9337/10174 [2:47:55<53:21,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9337, attempt 1: Expecting value: line 7 column 28 (char 227)\n",
      "‚ùå Error at row 9337, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9338/10174 [2:48:01<59:21,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9338, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9338, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9338, attempt 3: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9341/10174 [2:48:11<49:55,  3.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9341, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9341, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9345/10174 [2:48:26<47:16,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9345, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9351/10174 [2:48:45<39:48,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9351, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9351, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9353/10174 [2:48:52<43:11,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9353, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9353, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9353, attempt 3: No JSON block found\n",
      "‚ùå Error at row 9353, attempt 4: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9354/10174 [2:48:59<56:20,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9353, attempt 5: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9356/10174 [2:49:04<44:48,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9356, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9356, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9356, attempt 3: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9357/10174 [2:49:10<58:48,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9357, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9358/10174 [2:49:14<55:35,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9358, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9359/10174 [2:49:17<52:56,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9359, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9361/10174 [2:49:23<45:31,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9361, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9361, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9361, attempt 3: No JSON block found\n",
      "‚ùå Error at row 9361, attempt 4: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9365/10174 [2:49:40<49:20,  3.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9365, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9365, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9366/10174 [2:49:45<53:21,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9366, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9369/10174 [2:49:56<49:42,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9369, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9370/10174 [2:50:00<49:36,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9370, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9370, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9382/10174 [2:50:32<33:59,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9382, attempt 1: Expecting value: line 8 column 28 (char 319)\n",
      "‚ùå Error at row 9382, attempt 2: Expecting value: line 8 column 27 (char 302)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9385/10174 [2:50:52<1:01:49,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9385, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9385, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9386/10174 [2:50:57<1:02:51,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9386, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9390/10174 [2:51:08<40:52,  3.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9390, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9395/10174 [2:51:23<38:49,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9395, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9400/10174 [2:51:37<34:02,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9400, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9403/10174 [2:51:45<33:36,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9403, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9403, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9403, attempt 3: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9408/10174 [2:52:01<34:45,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9408, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9410/10174 [2:52:07<36:05,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9410, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9410, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9412/10174 [2:52:14<40:27,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9412, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9412, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9416/10174 [2:52:28<39:37,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9416, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9416, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9416, attempt 3: No JSON block found\n",
      "‚ùå Error at row 9416, attempt 4: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9417/10174 [2:52:33<46:06,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9416, attempt 5: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9419/10174 [2:52:38<39:37,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9419, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9424/10174 [2:52:55<40:34,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9424, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9425/10174 [2:52:59<42:43,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9425, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9427/10174 [2:53:07<48:06,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9427, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9428/10174 [2:53:11<46:15,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9428, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9431/10174 [2:53:19<37:38,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9431, attempt 1: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9436/10174 [2:53:33<32:54,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9436, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9436, attempt 2: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9438/10174 [2:53:41<38:10,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9438, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9438, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9438, attempt 3: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9440/10174 [2:53:49<43:58,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9440, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9440, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9440, attempt 3: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9441/10174 [2:53:55<52:16,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error at row 9441, attempt 1: No JSON block found\n",
      "‚ùå Error at row 9441, attempt 2: No JSON block found\n",
      "‚ùå Error at row 9441, attempt 3: No JSON block found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9445/10174 [2:54:09<40:27,  3.33s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_file)\n",
    "llm_fields = [\n",
    "    \"llm_length_effort\", \"llm_lexical_diversity\", \"llm_questions_raised\",\n",
    "    \"llm_citation_usage\", \"llm_sentiment_polarity\", \"llm_politeness\", \"llm_hedging\",\n",
    "    \"llm_specificity\", \"llm_domain_terms\", \"llm_relevance_alignment\",\n",
    "    \"llm_readability\", \"llm_overall_quality\", \"llm_overall_score_100\"\n",
    "]\n",
    "\n",
    "# Check for missing fields and add them if not present\n",
    "for field in llm_fields:\n",
    "    if field not in df.columns:\n",
    "        df[field] = pd.NA\n",
    "\n",
    "# Pattern to extract JSON block\n",
    "pattern = re.compile(r\"<review_assessment>\\s*(\\{.*?\\})\\s*</review_assessment>\", re.DOTALL)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"# REVIEW-QUALITY JUDGE\n",
    "\n",
    "## 0 ‚Äî ROLE  \n",
    "You are **ReviewInspector-LLM**, a rigorous, impartial meta-reviewer.  \n",
    "Your goal is to grade the quality of a single peer-review against a rich set of predefined criteria and to provide concise, actionable feedback.\n",
    "\n",
    "## 1 ‚Äî INPUTS  \n",
    "Title : {title}  \n",
    "Abstract : {abstract}  \n",
    "Review: {review_text}\n",
    "\n",
    "## 2 ‚Äî EVALUATION CRITERIA  \n",
    "Return **only** the scale value or label at right (no rationale text).\n",
    "\n",
    "| # | Criterion | Allowed scale / label |\n",
    "|---|-----------|-----------------------|\n",
    "| 1 | **Length & Effort**             | integer **0-5** |\n",
    "| 2 | **Lexical Diversity (TTR)**     | integer **0-5** |\n",
    "| 3 | **Number of Questions Raised**  | non-negative **integer** |\n",
    "| 4 | **Citation Usage**              | **yes / no** |\n",
    "| 5 | **Sentiment Polarity**          | **negative / neutral / positive** |\n",
    "| 6 | **Politeness**                  | **polite / neutral / impolite** |\n",
    "| 7 | **Hedging / Uncertainty**       | **No Hedging / Minimal / Moderate / Heavy / Extreme** |\n",
    "| 8 | **Specificity**                 | **very specific / somewhat specific / neutral / broad / very broad** |\n",
    "| 9 | **Use of Domain-Specific Terms**| integer **0-5** |\n",
    "|10 | **Relevance Alignment**         | integer **0-5** |\n",
    "|11 | **Readability**                 | integer **0-5** |\n",
    "|12 | **Overall Quality**             | integer **0-100** |\n",
    "\n",
    "## 3 ‚Äî SCORING GUIDELINES  \n",
    "For 0-5 scales: 5 = Outstanding, 4 = Strong, 3 = Adequate, 2 = Weak, 1 = Very weak, 0 = Absent/irrelevant.\n",
    "\n",
    "## 4 ‚Äî ANALYSIS & COMPUTATION (silent)  \n",
    "1. Read and comprehend the review text.  \n",
    "2. Compute raw metrics (word count, TTR, sentiment, FK grade, counts, etc.) or otherwise quantify qualitative aspects.  \n",
    "3. Map raw metrics to the scales above.\n",
    "\n",
    "## 5 ‚Äî OUTPUT FORMAT (strict)  \n",
    "Return **exactly one** JSON block wrapped in the tag below ‚Äî **no comments or extra text**.\n",
    "\n",
    "```json\n",
    "<review_assessment>\n",
    "{{\n",
    "  \"paper_title\": \"{title}\",\n",
    "  \"criteria\": {{\n",
    "    \"length_effort\":       ...,\n",
    "    \"lexical_diversity\":   ...,\n",
    "    \"questions_raised\":    ...,\n",
    "    \"citation_usage\":      ...,\n",
    "    \"sentiment_polarity\":  ...,\n",
    "    \"politeness\":          ...,\n",
    "    \"hedging\":             ...,\n",
    "    \"specificity\":         ...,\n",
    "    \"domain_terms\":        ...,\n",
    "    \"relevance_alignment\": ...,\n",
    "    \"readability\":         ...,\n",
    "    \"overall_quality\":     ...\n",
    "  }},\n",
    "  \"overall_score_100\": ...\n",
    "}}\n",
    "</review_assessment>\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring with LLM\"):\n",
    "    # Skip if all llm fields are already filled\n",
    "    if all(pd.notna(row.get(field, pd.NA)) for field in llm_fields):\n",
    "        continue\n",
    "\n",
    "    prompt = template.format(\n",
    "        title=row['title'],\n",
    "        abstract=row['abstract'],\n",
    "        review_text=row['review_text']\n",
    "    )\n",
    "\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = chat(\"llama3.2\", messages=[{'role': 'user', 'content': prompt}])\n",
    "            content = response['message']['content']\n",
    "            match = pattern.search(content)\n",
    "            if not match:\n",
    "                raise ValueError(\"No JSON block found\")\n",
    "\n",
    "            parsed = json.loads(match.group(1))\n",
    "            for key, val in parsed[\"criteria\"].items():\n",
    "                df.at[idx, f\"llm_{key}\"] = val\n",
    "            df.at[idx, \"llm_overall_score_100\"] = parsed[\"overall_score_100\"]\n",
    "\n",
    "            # Save after every successful row\n",
    "            df.to_csv(input_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at row {idx}, attempt {attempt + 1}: {e}\")\n",
    "            # time.sleep(0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
